llm:
  provider: OPENAI

  meta:
    model: mistral
    max_tokens: 2048
    temperature: 0.2

  http_client:
    timeout: 600
    api_url: http://localhost:11434

prompt:
  normalize_prompts: true
  summary_prompt_files: [ ./prompts/summary.md ]

review:
  mode: ADDED_AND_REMOVED_WITH_CONTEXT
  inline_tag: "#ai-review:inline"
  inline_reply_tag: "#ai-review:inline-reply"
  summary_tag: "#ai-review:summary"
  summary_reply_tag: "#ai-review:summary-reply"
  context_lines: 50

vcs:
  provider: GITHUB

  http_client:
    timeout: 120
    api_url: https://api.github.com
    
